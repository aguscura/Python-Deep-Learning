{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Como se entrena la red.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMptvGdPweTkr8XSW71O+Oj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguscura/Python-Deep-Learning/blob/main/Como_se_entrena_la_red.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Como se entrena una red neuronal**"
      ],
      "metadata": {
        "id": "TTFSKRRscT6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9JEWI88M-iMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para controlar la salida de una red neuronal, debe poder medirse cuan lejos está la salida de lo que se esperaba. Este es el trabajo de la función de perdida de la red (**loss function**). La funcion de perdida toma las predicciones de la red y el valor real de la etiqueta y calcula un error cometido para una muestra de entrada específica.\n",
        "\n",
        "Este calculo del error sirve como **retroalimentacion** para la red *ajustando los parámetros de manera que el error disminuya gradualmente.*\n",
        "\n",
        "Este **ajuste** es realizado por el **optimizador** que usa esta ***retropropagación*** del error para el ajuste de los parámetros. Esto es clave para el aprendizaje y se denomina \"Backpropagation\"."
      ],
      "metadata": {
        "id": "1q8l5OaOcZpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward propagation y back propagation**\n",
        "\n",
        "*Forward propagation: * Se da cuando los datos de entrada cruzan toda la red en el camino de ida. Estos inputs son afectados por los parámetros de la red (pesos y sesgos) para calcular las supuestas etiquetas (labels) de cada imágen.\n",
        "\n",
        "Luego, se calcula el error a traves de la función de perdida, interviene el optimizador para ir disminuyendo ese error, se ajustan los pesos en un proceso de retroalimentación llamado *backward propagation*.\n",
        "\n",
        "*Algo importante a tener en cuenta es que las neuronas de cada capa solo reciben una fracción de la señal total de error. Esta fracción se basa en la contribución relativa que haya aportado cada neurona a la salida original. Esto se repite hacia atras capa por capa hasta que las neuronas de todas las capas de la red hayan recibido una señal del error cometido.*"
      ],
      "metadata": {
        "id": "y_8DXeQF7iB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El optimizador usa una técnica llamada **Gradient Descent** la cual va cambiando los pesos en pequeños incrementos calculando la derivada de la funcion de perdida. Esto nos permite ver en que direccion debemos movernos para llegar al minimo global."
      ],
      "metadata": {
        "id": "TeLVYZe485st"
      }
    }
  ]
}