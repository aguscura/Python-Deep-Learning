{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Como se entrena la red.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMunJ6SdMiquoChlV83GDt9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguscura/Python-Deep-Learning/blob/main/Como_se_entrena_la_red.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Como se entrena una red neuronal**"
      ],
      "metadata": {
        "id": "TTFSKRRscT6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para controlar la salida de una red neuronal, debe poder medirse cuan lejos está la salida de lo que se esperaba. Este es el trabajo de la función de perdida de la red (**loss function**). La funcion de perdida toma las predicciones de la red y el valor real de la etiqueta y calcula un error cometido para una muestra de entrada específica.\n",
        "\n",
        "Este calculo del error sirve como **retroalimentacion** para la red *ajustando los parámetros de manera que el error disminuya gradualmente.*\n",
        "\n",
        "Este **ajuste** es realizado por el **optimizador** que usa esta ***retropropagación*** del error para el ajuste de los parámetros. Esto es clave para el aprendizaje y se denomina \"Backpropagation\"."
      ],
      "metadata": {
        "id": "1q8l5OaOcZpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward propagation y back propagation**\n",
        "\n",
        "*Forward propagation: * Se da cuando los datos de entrada cruzan toda la red en el camino de ida. Estos inputs son afectados por los parámetros de la red (pesos y sesgos) para calcular las supuestas etiquetas (labels) de cada imágen.\n",
        "\n",
        "Luego, se calcula el error a traves de la función de perdida, interviene el optimizador para ir disminuyendo ese error, se ajustan los pesos en un proceso de retroalimentación llamado *backward propagation*.\n",
        "\n",
        "*Algo importante a tener en cuenta es que las neuronas de cada capa solo reciben una fracción de la señal total de error. Esta fracción se basa en la contribución relativa que haya aportado cada neurona a la salida original. Esto se repite hacia atras capa por capa hasta que las neuronas de todas las capas de la red hayan recibido una señal del error cometido.* A esto se le llama **diferenciación simbolica**."
      ],
      "metadata": {
        "id": "y_8DXeQF7iB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El optimizador usa una técnica llamada **Gradient Descent** la cual va cambiando los pesos en pequeños incrementos calculando la derivada de la funcion de perdida. Esto nos permite ver en que direccion debemos movernos para llegar al minimo global."
      ],
      "metadata": {
        "id": "TeLVYZe485st"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent"
      ],
      "metadata": {
        "id": "nG2_M_FK-qZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al igual que la derivada, el gradiente representa la pendiente de la recta tangente a la gráfica de una función. Más precisamente, el gradiente apunta a los puntos de la gráfica a los cuales la gráfica **tiene un mayor incremento**. La magnitud del gradiente es la pendiente de la gráfica en esa dirección."
      ],
      "metadata": {
        "id": "tBPVV7BcBY2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cada iteración, **cada neurona** obtiene el valor del gradiente de la funcion de perdida que le corresponde. Luego, se actualizan los valores en la dirección opuesta al gradiente. Esto se da ya que el gradiente apunta en la dirección de max. crecimiento de la función de perdida. Por lo tanto, al hacer el negativo del gradiente, estamos yendo en la dirección de max. decrecimiento y por lo tanto disminuimos el error."
      ],
      "metadata": {
        "id": "PksB_dWFCGe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DKwEv9NZE84H"
      }
    }
  ]
}