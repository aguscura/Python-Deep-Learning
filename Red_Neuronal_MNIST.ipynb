{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Red Neuronal MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO5lKDt3Qeib+FUC1PRAkYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguscura/Python-Deep-Learning/blob/main/Red_Neuronal_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamiento (Train)\n",
        "#Prueba (Test)\n",
        "#Validación"
      ],
      "metadata": {
        "id": "HaBEYzjiSZH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "PoLdiEvASfB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyvGEmf5RWs3"
      },
      "outputs": [],
      "source": [
        "#Precarga de datos, la base MNIST viene por defecto en Keras.\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(x_train[8], cmap=plt.cm.binary)\n",
        "\n",
        "\"\"\" cmap: Colormap.\n",
        "    Default: rcParams[\"image.cmap\"] (default: 'viridis')\n",
        "    The Colormap instance or registered colormap name used to map scalar data to colors. This parameter is ignored for RGB(A) data.\n",
        "                                                                                                                                    \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "x_UqRtj9S-8m",
        "outputId": "b2be3118-8a7d-44a0-cba3-297e81841ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' cmap: Colormap.\\n    Default: rcParams[\"image.cmap\"] (default: \\'viridis\\')\\n    The Colormap instance or registered colormap name used to map scalar data to colors. This parameter is ignored for RGB(A) data.\\n                                                                                                                                    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBfGzwDZT1RO",
        "outputId": "65514c50-716c-416f-8105-2a63b96c5e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimensiones del tensor x_train\n",
        "print(x_train.ndim)\n",
        "\n",
        "#Forma del tensor x_train\n",
        "print(x_train.shape)\n",
        "\n",
        "#Tipo de datos del tensor x_train\n",
        "print(x_train.dtype)\n",
        "\n",
        "\"\"\" x_train es un tensor 3D de enteros de 8bits. \n",
        "    Concretamente, es un VECTOR de 60 mil matrices 2D de 28x28 enteros. \n",
        "    Cada una de esas matrices es una imagen en escala de grises con coeficientes entre 0 y 255.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "t-ZnmMHaT-_Y",
        "outputId": "63333412-604a-48fd-f875-c25c0b938aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "(60000, 28, 28)\n",
            "uint8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' x_train es un tensor 3D de enteros de 8bits. \\n    Concretamente, es un VECTOR de 60 mil matrices 2D de 28x28 enteros. \\n    Cada una de esas matrices es una imagen en escala de grises con coeficientes entre 0 y 255.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE** (Página 101).\n",
        "En este ejemplo estamos tratando un ejemplo en blanco y negro, pero en general una imagen en color suele tener 3 dimensiones. Altura, anchura y profundidad de color. Las imágenes en escala de grises (como los digitos MNIST) tienen un solo canal de color y, por lo tanto, pueden almacenarse en tensores 2D CADA UNA DE ELLAS. Pero habitualmente los tensores de las imágenes son 3D. (O sea, cada imagen individualmente es un tensor 3D). \n",
        "\n",
        "Por ejemplo, 64 imágenes en escala de grises de tamaño 128x128 podrían almacenarse en un tensor de la forma (64, 128, 128, 1). En cambio, un conjunto de 64 imágenes en color de tamaño 128x128 podría almacenarse en un tensor de la forma (64, 128, 128, 3). Esos 3 canales son para el uso de la codificacion RGB. En ambos casos hablamos de un tensor 4D con la forma (samples, height, width, channels).\n",
        "\n",
        "Repaso: Cada imágen podemos generalizarla como un tensor 3D. Es decir, un cubo.\n",
        "Ese cubo tendrá 128x128 (alto y ancho) y 3 de espesor. Cada una de esas 3 matrices, representa las intensidades en Red, Green y Blue.\n",
        "Encima, tenemos 64 cubos por lo que finalmente terminamos teniendo un vector de 64 cubos.\n",
        "Para escala de grises, tenemos un solo canal de color, entonces no tenemos cubos. En su lugar tenemos matrices directamente como el ejemplo inicial. \n",
        "\n",
        "Nota: Cualquier conjunto de datos se puede representar como un tensor. Por ejemplo un video es una secuencia de fotos (fotogramas, frames). Podemos representarlo en un tensor 5D: (samples, frames, height, width, channels)."
      ],
      "metadata": {
        "id": "JPCLQNleVd0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Manipular un tensor: Vamos a crear un mini tensor con las primeras 100 imagenes del anterior que contenía 60 mil.\n",
        "\n",
        "first_100 = x_train[0:100,:,:]\n",
        "print(first_100.shape)\n",
        "\n",
        "#EQUIVALENTE!!!\n",
        "first_100_bis = x_train[0:100, 0:28, 0:28]\n",
        "print(first_100_bis.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UbMDS5pYpon",
        "outputId": "3f7e852a-f117-4187-b3ae-eeeb60a10136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 28, 28)\n",
            "(100, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ahora, en vez de recortar la cantidad de imagenes, recorto los pixeles. \n",
        "#Tomo esquina INFERIOR DERECHA\n",
        "\n",
        "inferior_derecha = x_train[:, 14:, 14:]\n",
        "print(inferior_derecha.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlQAa5XcF2zz",
        "outputId": "f678a4ee-6598-42ad-d8a9-d67a3ab2496c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 14, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[11].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zx35HmcMSHv",
        "outputId": "78b80c0c-6874-4fc4-938c-589b3cc2624b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Idem anterior, pero cortando la PARTE CENTRAL de la imagen. Usando indices relativos:\n",
        "#Nota, el recorte también es de 14x14 pixeles en este caso.\n",
        "\n",
        "centro_imagenes = x_train[:, 7:-7, 7:-7]\n",
        "centro_imagenes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z75etx9AGP7q",
        "outputId": "ab735f31-8a60-42a7-c9a9-ce6086025847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 14, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPROCESAMIENTO"
      ],
      "metadata": {
        "id": "ts72YfItGp3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo del preprocesamiento de datos es adaptar los mismos a un formato que permita un mejor aprovechamiento por parte de las redes neuronales.\n",
        "Los ejemplos más comunes para imagenes y redes neuronales son:\n",
        "**VECTORIZACION,\n",
        "NORMALIZACION,\n",
        "EXTRACCION DE CARACTERISTICAS**"
      ],
      "metadata": {
        "id": "X8gGfDBZG2Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NORMALIZACION - Las matrices de 28x28 contienen numeros que van del 0 al 255 de tipo Uint8. Conviene normalizarlos a float32 en el rango de 0 a 1. \n",
        "# Esto se hace dividiendo toda la matriz por 255. \n",
        "\n",
        "\n",
        "# 1ro cambio de tipo\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\"\"\" Este tipo de normalizacion se hace para facilitar que converja el proceso de entrenamiento. Porque, en general, para alimentar una red no se usan datos con valores que sean mucho mas grandes\n",
        "que los valores de los pesos de la red \"\"\""
      ],
      "metadata": {
        "id": "3gCB_KQfHSao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "86a71d75-07b7-4c77-a171-990ca60b16fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Este tipo de normalizacion se hace para facilitar que converja el proceso de entrenamiento. Porque, en general, para alimentar una red no se usan datos con valores que sean mucho mas grandes\\nque los valores de los pesos de la red '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CAMBIO DE FORMA (RESHAPE)\n",
        "\"\"\" Cambiar la forma de los tensores sin cambiar los datos. Para facilitar la entrada de los datos a la red neuronal, se transforma el tensor de una imagen 2D (matriz de 28x28) a un vector de 1D. \n",
        "Es decir, la matriz de 28x28 numeros se puede representar como un vector de 784 numeros (concatenando fila a fila). Este es el formato de entrada que acepta la red neuronal densamente conectada.\n",
        "\"\"\"\n",
        "\n",
        "#En el siguiente ejemplo, la primera dimension indexa la imagen y la segunda dimension indexa el píxel en cada imagen.\n",
        "x_train = x_train.reshape(60000,784)\n",
        "x_test = x_test.reshape(10000,784)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZzfS1yfIXdq",
        "outputId": "f7b7ac94-bc6d-465c-e9d9-c12c0e255c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora vamos a trabajar con el vector de las labels. El vector y. \n",
        "# Hacemos un one hot encoding. Para cada imagen, el vector de salida va a tener 10 posiciones. 9 van a ser ceros. El 1 estará en la etiqueta correcta.\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#Valor antes de aplicar la transformación para la primer imagen:\n",
        "print(y_train[0])\n",
        "print(y_test[0])\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6mTq71JJsXE",
        "outputId": "38b61f34-1b69-474e-aa08-85776bfd350f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "7\n",
            "(60000,)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJe-55ABN0Dq",
        "outputId": "786971df-df25-45e2-c483-f8ce9ee5b00d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Valor despues de aplicar la transformacion para la primer imagen\n",
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrarz0DMN60l",
        "outputId": "49d4e7e0-de86-48f8-f64d-b901e02750da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LISTO, YA TENEMOS LOS DATOS EN EL FORMATO QUE NECESITAMOS PARA USARLOS EN NUESTRO MODELO.**"
      ],
      "metadata": {
        "id": "kki-MWa5OPjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación del modelo básico usando 'Sequential'. Esta clase permite la creación de una red neuronal básica\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='sigmoid', input_shape=(784,)))\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "#Parámetros de las capas Dense --> Ver documentación\n",
        "#Un parámetro importante puede ser los valores de INICIO de los PESOS (W). Los podemos inicializar con un cierto valor para que el problema converja rápido.\n",
        "\n",
        "\"\"\" En este caso, la red neuronal se ha definido como una secuencia de dos capas densas que están COMPLETAMENTE CONECTADAS (todas las neuronas de la primera capa están conectadas con las de\n",
        "la siguiente capa). \n",
        "    El input_shape indica explicitamente cómo son los datos de entrada: un tensor que indica que tenemos 784 features. Cada pixel de una imagen es una caracteristica.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "lfqI4KW9OVlS",
        "outputId": "2ec6d4b4-d82d-4871-e8ce-edc4cabca564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' En este caso, la red neuronal se ha definido como una secuencia de dos capas densas que están COMPLETAMENTE CONECTADAS (todas las neuronas de la primera capa están conectadas con las de\\nla siguiente capa). \\n    El input_shape indica explicitamente cómo son los datos de entrada: un tensor que indica que tenemos 784 features. Cada pixel de una imagen es una caracteristica.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EN GENERAL, LA CAPA DE SALIDA DE UNA RED DE CLASIFICACION TENDRA TANTAS NEURONAS COMO CLASES. MENOS EN UNA CLASIFICACION BINARIA EN DONDE SOLO SE NECESITARÁ UNA NEURONA**"
      ],
      "metadata": {
        "id": "G-YIfeTpXJnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UTIL: Para ver la forma de un modelo\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQJzQ7WgXbBT",
        "outputId": "88340005-873f-454a-a950-93b682a3de05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 10)                7850      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,960\n",
            "Trainable params: 7,960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function** (Funcion de coste) --> Sirve para evaluar el error entre las salidas calculadas y las salidas deseadas de los datos de entrenamiento. Ejemplo: categorical_crossentropy\n",
        "\n",
        "** Optimizador ** Manera que tenemos de ir mejorando y reduciendo el error (loss function) para acercar los pesos (w) al optimo.\n",
        "\n",
        "** Métrica ** Qué metrica buscamos optimizar? Ejemplo Accuracy (precisión): Fracción de las imágenes que son correctamente clasificadas."
      ],
      "metadata": {
        "id": "9FQbz5_jWkCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "K1gn9USDWs0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**epochs** --> Cantidad de veces que usaremos todos los datos en el proceso de aprendizaje."
      ],
      "metadata": {
        "id": "wSjv_oRCktja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamiento propiamente dicho\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKKLyXRBkzmv",
        "outputId": "20151806-5e53-44d2-b2b3-a5435e95a978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.9009 - accuracy: 0.5098\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.3073 - accuracy: 0.6953\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9849 - accuracy: 0.7765\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.8007 - accuracy: 0.8174\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.6842 - accuracy: 0.8429\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f499b24fb50>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluamos el modelo. \"evaluate() devuelve 2 parametros: loss y accuracy\"\n",
        "\n",
        "validate_loss, validate_acc = model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpAcRwxhmDwY",
        "outputId": "33266a10-1537-49b7-fa5f-26c8ccc126ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 1ms/step - loss: 0.6252 - accuracy: 0.8607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy = Precisión. Qué porcentaje de los datos clasifica correctamente. En este caso el 87% de los registros del set de testeo fueron clasificados correctamente. \n",
        "#Accuracy = Predicciones correctas/ Total de las predicciones\n",
        "\n",
        "print(\"Accuracy\", validate_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpf2PQXnmPfl",
        "outputId": "ecfb3ac3-535a-402d-920a-8d00dcbb6e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.8607000112533569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Recall = Indica cómo de bien el modelo evita los falsos negativos. Los falsos negativos son MUY importantes. Podría ser trágico si decis que una imagen no es melanoma y efectivamente lo era.\n",
        "#Recall = Se calcula sobre todos los true_positives. Es decir (melanomas predichos correctamente / todos los melanomas )\n",
        "#Recall = VP / (VP + FN)\n",
        "\n",
        "\"\"\" La conveniencia de usar una métrica u otra dependerá de cada caso en particular y del COSTE asociado a cada error de clasificacion del modelo \"\"\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yz3vU5PHGq4y",
        "outputId": "6011cd00-4abb-47cd-a5db-795cf545e662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' La conveniencia de usar una métrica u otra dependerá de cada caso en particular y del COSTE asociado a cada error de clasificacion del modelo '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ahora vamos a realizar una predicción sobre una imagen determinada usando el modelo ya entrenado\n",
        "#Imagen que vamos a utilizar:\n",
        "\n",
        "\n",
        "#Pude hacer el reshape de todo el array de imágenes. No de la imágen individual.\n",
        "#Le hacemos reshape a 28x28 para ver la imagen con imshow.\n",
        "\n",
        "x_test_original = x_test.reshape(10000,28,28)\n",
        "print(x_test_original[11].shape)\n",
        "\n",
        "plt.imshow(x_test_original[11], cmap=plt.cm.binary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "QPVn0N4OINt8",
        "outputId": "c90fcc92-6325-4c53-ad43-055117090f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f499fc24310>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicciones\n",
        "import numpy as np\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "print(predictions[11]) # Probabilidades para la imagen 11.\n",
        "print(np.argmax(predictions[11])) #Indice de la posición que contiene el valor más alto.\n",
        "print(np.sum(predictions[11])) #Suma de todas las probabilidades. Debe ser igual a 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPsbzdCcvKnt",
        "outputId": "8852a9f2-740a-4ff4-a17d-990ea0598462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.04782686 0.01966085 0.11770047 0.05761197 0.02621655 0.07210248\n",
            " 0.54781944 0.00195621 0.10050131 0.00860388]\n",
            "6\n",
            "1.0000001\n"
          ]
        }
      ]
    }
  ]
}