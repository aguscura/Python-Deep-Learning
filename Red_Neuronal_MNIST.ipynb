{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Red Neuronal MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO5lKDt3Qeib+FUC1PRAkYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguscura/Python-Deep-Learning/blob/main/Red_Neuronal_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamiento (Train)\n",
        "#Prueba (Test)\n",
        "#Validaci√≥n"
      ],
      "metadata": {
        "id": "HaBEYzjiSZH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "PoLdiEvASfB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyvGEmf5RWs3"
      },
      "outputs": [],
      "source": [
        "#Precarga de datos, la base MNIST viene por defecto en Keras.\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(x_train[8], cmap=plt.cm.binary)\n",
        "\n",
        "\"\"\" cmap: Colormap.\n",
        "    Default: rcParams[\"image.cmap\"] (default: 'viridis')\n",
        "    The Colormap instance or registered colormap name used to map scalar data to colors. This parameter is ignored for RGB(A) data.\n",
        "                                                                                                                                    \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "x_UqRtj9S-8m",
        "outputId": "b2be3118-8a7d-44a0-cba3-297e81841ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' cmap: Colormap.\\n    Default: rcParams[\"image.cmap\"] (default: \\'viridis\\')\\n    The Colormap instance or registered colormap name used to map scalar data to colors. This parameter is ignored for RGB(A) data.\\n                                                                                                                                    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBfGzwDZT1RO",
        "outputId": "65514c50-716c-416f-8105-2a63b96c5e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimensiones del tensor x_train\n",
        "print(x_train.ndim)\n",
        "\n",
        "#Forma del tensor x_train\n",
        "print(x_train.shape)\n",
        "\n",
        "#Tipo de datos del tensor x_train\n",
        "print(x_train.dtype)\n",
        "\n",
        "\"\"\" x_train es un tensor 3D de enteros de 8bits. \n",
        "    Concretamente, es un VECTOR de 60 mil matrices 2D de 28x28 enteros. \n",
        "    Cada una de esas matrices es una imagen en escala de grises con coeficientes entre 0 y 255.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "t-ZnmMHaT-_Y",
        "outputId": "63333412-604a-48fd-f875-c25c0b938aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "(60000, 28, 28)\n",
            "uint8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' x_train es un tensor 3D de enteros de 8bits. \\n    Concretamente, es un VECTOR de 60 mil matrices 2D de 28x28 enteros. \\n    Cada una de esas matrices es una imagen en escala de grises con coeficientes entre 0 y 255.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE** (P√°gina 101).\n",
        "En este ejemplo estamos tratando un ejemplo en blanco y negro, pero en general una imagen en color suele tener 3 dimensiones. Altura, anchura y profundidad de color. Las im√°genes en escala de grises (como los digitos MNIST) tienen un solo canal de color y, por lo tanto, pueden almacenarse en tensores 2D CADA UNA DE ELLAS. Pero habitualmente los tensores de las im√°genes son 3D. (O sea, cada imagen individualmente es un tensor 3D). \n",
        "\n",
        "Por ejemplo, 64 im√°genes en escala de grises de tama√±o 128x128 podr√≠an almacenarse en un tensor de la forma (64, 128, 128, 1). En cambio, un conjunto de 64 im√°genes en color de tama√±o 128x128 podr√≠a almacenarse en un tensor de la forma (64, 128, 128, 3). Esos 3 canales son para el uso de la codificacion RGB. En ambos casos hablamos de un tensor 4D con la forma (samples, height, width, channels).\n",
        "\n",
        "Repaso: Cada im√°gen podemos generalizarla como un tensor 3D. Es decir, un cubo.\n",
        "Ese cubo tendr√° 128x128 (alto y ancho) y 3 de espesor. Cada una de esas 3 matrices, representa las intensidades en Red, Green y Blue.\n",
        "Encima, tenemos 64 cubos por lo que finalmente terminamos teniendo un vector de 64 cubos.\n",
        "Para escala de grises, tenemos un solo canal de color, entonces no tenemos cubos. En su lugar tenemos matrices directamente como el ejemplo inicial. \n",
        "\n",
        "Nota: Cualquier conjunto de datos se puede representar como un tensor. Por ejemplo un video es una secuencia de fotos (fotogramas, frames). Podemos representarlo en un tensor 5D: (samples, frames, height, width, channels)."
      ],
      "metadata": {
        "id": "JPCLQNleVd0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Manipular un tensor: Vamos a crear un mini tensor con las primeras 100 imagenes del anterior que conten√≠a 60 mil.\n",
        "\n",
        "first_100 = x_train[0:100,:,:]\n",
        "print(first_100.shape)\n",
        "\n",
        "#EQUIVALENTE!!!\n",
        "first_100_bis = x_train[0:100, 0:28, 0:28]\n",
        "print(first_100_bis.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UbMDS5pYpon",
        "outputId": "3f7e852a-f117-4187-b3ae-eeeb60a10136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 28, 28)\n",
            "(100, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ahora, en vez de recortar la cantidad de imagenes, recorto los pixeles. \n",
        "#Tomo esquina INFERIOR DERECHA\n",
        "\n",
        "inferior_derecha = x_train[:, 14:, 14:]\n",
        "print(inferior_derecha.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlQAa5XcF2zz",
        "outputId": "f678a4ee-6598-42ad-d8a9-d67a3ab2496c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 14, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[11].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zx35HmcMSHv",
        "outputId": "78b80c0c-6874-4fc4-938c-589b3cc2624b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Idem anterior, pero cortando la PARTE CENTRAL de la imagen. Usando indices relativos:\n",
        "#Nota, el recorte tambi√©n es de 14x14 pixeles en este caso.\n",
        "\n",
        "centro_imagenes = x_train[:, 7:-7, 7:-7]\n",
        "centro_imagenes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z75etx9AGP7q",
        "outputId": "ab735f31-8a60-42a7-c9a9-ce6086025847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 14, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPROCESAMIENTO"
      ],
      "metadata": {
        "id": "ts72YfItGp3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo del preprocesamiento de datos es adaptar los mismos a un formato que permita un mejor aprovechamiento por parte de las redes neuronales.\n",
        "Los ejemplos m√°s comunes para imagenes y redes neuronales son:\n",
        "**VECTORIZACION,\n",
        "NORMALIZACION,\n",
        "EXTRACCION DE CARACTERISTICAS**"
      ],
      "metadata": {
        "id": "X8gGfDBZG2Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NORMALIZACION - Las matrices de 28x28 contienen numeros que van del 0 al 255 de tipo Uint8. Conviene normalizarlos a float32 en el rango de 0 a 1. \n",
        "# Esto se hace dividiendo toda la matriz por 255. \n",
        "\n",
        "\n",
        "# 1ro cambio de tipo\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\"\"\" Este tipo de normalizacion se hace para facilitar que converja el proceso de entrenamiento. Porque, en general, para alimentar una red no se usan datos con valores que sean mucho mas grandes\n",
        "que los valores de los pesos de la red \"\"\""
      ],
      "metadata": {
        "id": "3gCB_KQfHSao",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "86a71d75-07b7-4c77-a171-990ca60b16fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Este tipo de normalizacion se hace para facilitar que converja el proceso de entrenamiento. Porque, en general, para alimentar una red no se usan datos con valores que sean mucho mas grandes\\nque los valores de los pesos de la red '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CAMBIO DE FORMA (RESHAPE)\n",
        "\"\"\" Cambiar la forma de los tensores sin cambiar los datos. Para facilitar la entrada de los datos a la red neuronal, se transforma el tensor de una imagen 2D (matriz de 28x28) a un vector de 1D. \n",
        "Es decir, la matriz de 28x28 numeros se puede representar como un vector de 784 numeros (concatenando fila a fila). Este es el formato de entrada que acepta la red neuronal densamente conectada.\n",
        "\"\"\"\n",
        "\n",
        "#En el siguiente ejemplo, la primera dimension indexa la imagen y la segunda dimension indexa el p√≠xel en cada imagen.\n",
        "x_train = x_train.reshape(60000,784)\n",
        "x_test = x_test.reshape(10000,784)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZzfS1yfIXdq",
        "outputId": "f7b7ac94-bc6d-465c-e9d9-c12c0e255c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora vamos a trabajar con el vector de las labels. El vector y. \n",
        "# Hacemos un one hot encoding. Para cada imagen, el vector de salida va a tener 10 posiciones. 9 van a ser ceros. El 1 estar√° en la etiqueta correcta.\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#Valor antes de aplicar la transformaci√≥n para la primer imagen:\n",
        "print(y_train[0])\n",
        "print(y_test[0])\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6mTq71JJsXE",
        "outputId": "38b61f34-1b69-474e-aa08-85776bfd350f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "7\n",
            "(60000,)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJe-55ABN0Dq",
        "outputId": "786971df-df25-45e2-c483-f8ce9ee5b00d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Valor despues de aplicar la transformacion para la primer imagen\n",
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrarz0DMN60l",
        "outputId": "49d4e7e0-de86-48f8-f64d-b901e02750da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LISTO, YA TENEMOS LOS DATOS EN EL FORMATO QUE NECESITAMOS PARA USARLOS EN NUESTRO MODELO.**"
      ],
      "metadata": {
        "id": "kki-MWa5OPjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creaci√≥n del modelo b√°sico usando 'Sequential'. Esta clase permite la creaci√≥n de una red neuronal b√°sica\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='sigmoid', input_shape=(784,)))\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "#Par√°metros de las capas Dense --> Ver documentaci√≥n\n",
        "#Un par√°metro importante puede ser los valores de INICIO de los PESOS (W). Los podemos inicializar con un cierto valor para que el problema converja r√°pido.\n",
        "\n",
        "\"\"\" En este caso, la red neuronal se ha definido como una secuencia de dos capas densas que est√°n COMPLETAMENTE CONECTADAS (todas las neuronas de la primera capa est√°n conectadas con las de\n",
        "la siguiente capa). \n",
        "    El input_shape indica explicitamente c√≥mo son los datos de entrada: un tensor que indica que tenemos 784 features. Cada pixel de una imagen es una caracteristica.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "lfqI4KW9OVlS",
        "outputId": "2ec6d4b4-d82d-4871-e8ce-edc4cabca564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' En este caso, la red neuronal se ha definido como una secuencia de dos capas densas que est√°n COMPLETAMENTE CONECTADAS (todas las neuronas de la primera capa est√°n conectadas con las de\\nla siguiente capa). \\n    El input_shape indica explicitamente c√≥mo son los datos de entrada: un tensor que indica que tenemos 784 features. Cada pixel de una imagen es una caracteristica.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EN GENERAL, LA CAPA DE SALIDA DE UNA RED DE CLASIFICACION TENDRA TANTAS NEURONAS COMO CLASES. MENOS EN UNA CLASIFICACION BINARIA EN DONDE SOLO SE NECESITAR√Å UNA NEURONA**"
      ],
      "metadata": {
        "id": "G-YIfeTpXJnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UTIL: Para ver la forma de un modelo\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQJzQ7WgXbBT",
        "outputId": "88340005-873f-454a-a950-93b682a3de05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 10)                7850      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,960\n",
            "Trainable params: 7,960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function** (Funcion de coste) --> Sirve para evaluar el error entre las salidas calculadas y las salidas deseadas de los datos de entrenamiento. Ejemplo: categorical_crossentropy\n",
        "\n",
        "** Optimizador ** Manera que tenemos de ir mejorando y reduciendo el error (loss function) para acercar los pesos (w) al optimo.\n",
        "\n",
        "** M√©trica ** Qu√© metrica buscamos optimizar? Ejemplo Accuracy (precisi√≥n): Fracci√≥n de las im√°genes que son correctamente clasificadas."
      ],
      "metadata": {
        "id": "9FQbz5_jWkCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "K1gn9USDWs0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**epochs** --> Cantidad de veces que usaremos todos los datos en el proceso de aprendizaje."
      ],
      "metadata": {
        "id": "wSjv_oRCktja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Entrenamiento propiamente dicho\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKKLyXRBkzmv",
        "outputId": "20151806-5e53-44d2-b2b3-a5435e95a978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.9009 - accuracy: 0.5098\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 1.3073 - accuracy: 0.6953\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9849 - accuracy: 0.7765\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.8007 - accuracy: 0.8174\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.6842 - accuracy: 0.8429\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f499b24fb50>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluamos el modelo. \"evaluate() devuelve 2 parametros: loss y accuracy\"\n",
        "\n",
        "validate_loss, validate_acc = model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpAcRwxhmDwY",
        "outputId": "33266a10-1537-49b7-fa5f-26c8ccc126ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 1ms/step - loss: 0.6252 - accuracy: 0.8607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy = Precisi√≥n. Qu√© porcentaje de los datos clasifica correctamente. En este caso el 87% de los registros del set de testeo fueron clasificados correctamente. \n",
        "#Accuracy = Predicciones correctas/ Total de las predicciones\n",
        "\n",
        "print(\"Accuracy\", validate_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpf2PQXnmPfl",
        "outputId": "ecfb3ac3-535a-402d-920a-8d00dcbb6e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.8607000112533569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Recall = Indica c√≥mo de bien el modelo evita los falsos negativos. Los falsos negativos son MUY importantes. Podr√≠a ser tr√°gico si decis que una imagen no es melanoma y efectivamente lo era.\n",
        "#Recall = Se calcula sobre todos los true_positives. Es decir (melanomas predichos correctamente / todos los melanomas )\n",
        "#Recall = VP / (VP + FN)\n",
        "\n",
        "\"\"\" La conveniencia de usar una m√©trica u otra depender√° de cada caso en particular y del COSTE asociado a cada error de clasificacion del modelo \"\"\" "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Yz3vU5PHGq4y",
        "outputId": "6011cd00-4abb-47cd-a5db-795cf545e662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' La conveniencia de usar una m√©trica u otra depender√° de cada caso en particular y del COSTE asociado a cada error de clasificacion del modelo '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ahora vamos a realizar una predicci√≥n sobre una imagen determinada usando el modelo ya entrenado\n",
        "#Imagen que vamos a utilizar:\n",
        "\n",
        "\n",
        "#Pude hacer el reshape de todo el array de im√°genes. No de la im√°gen individual.\n",
        "#Le hacemos reshape a 28x28 para ver la imagen con imshow.\n",
        "\n",
        "x_test_original = x_test.reshape(10000,28,28)\n",
        "print(x_test_original[11].shape)\n",
        "\n",
        "plt.imshow(x_test_original[11], cmap=plt.cm.binary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "QPVn0N4OINt8",
        "outputId": "c90fcc92-6325-4c53-ad43-055117090f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f499fc24310>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicciones\n",
        "import numpy as np\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "print(predictions[11]) # Probabilidades para la imagen 11.\n",
        "print(np.argmax(predictions[11])) #Indice de la posici√≥n que contiene el valor m√°s alto.\n",
        "print(np.sum(predictions[11])) #Suma de todas las probabilidades. Debe ser igual a 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPsbzdCcvKnt",
        "outputId": "8852a9f2-740a-4ff4-a17d-990ea0598462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.04782686 0.01966085 0.11770047 0.05761197 0.02621655 0.07210248\n",
            " 0.54781944 0.00195621 0.10050131 0.00860388]\n",
            "6\n",
            "1.0000001\n"
          ]
        }
      ]
    }
  ]
}