{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parametros-e-Hiperparametros.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOaoVDyCHkbiQzvRgneBA9o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguscura/Python-Deep-Learning/blob/main/Parametros_e_Hiperparametros.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parámetros** --> Son internos a la red neuronal. Por ejemplo los pesos de las neuronas. Se aprenden automáticamente a partir de los datos de entrenamiento. Estos, son los mismos parametros que luego se usan en producción para hacer predicciones.\n",
        "\n",
        "**Hiperparámetros** --> Son externos a la red y los determina el programador. Ejemplo tamaño del batch, funciones de activación posibles, epochs, learning_rate, etc.\n",
        "\n",
        "Es importante aclarar que hoy en día ya hay propuestas para que un algoritmo busque directamente los hiperparámetros optimos por nosotros. Ejemplos **HyperOpt, KOpt, Talos, GPflowOpt**. Y en el cloud computing, Google Cloud. "
      ],
      "metadata": {
        "id": "C4FErHpd3i_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dos grandes grupos:\n",
        "\n",
        "Hiperparámetros a nivel **estructura y topologia** de la red: Número de capas, número de neuronas por capa, funciones de activación, inicialización de los pesos.\n",
        "\n",
        "Hiperparámetros a nivel de algoritmo de **aprendizaje**: Learning rate, epochs, batch size, momentum, etc."
      ],
      "metadata": {
        "id": "vZr8RfdN4ZCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hiperparámetros relacionados con el aprendizaje\n",
        "\n",
        "**Epochs**\n",
        "\n",
        "Numero de veces que los datos de entrenamiento han pasado por la red neuronal. Un numero en exceso de epochs hace que el modelo se ajuste en exceso a los datos (overfitting) lo que genera problemas de generalización (se le complica cuando ve algo nuevo despues. También un número excesivo de epochs puede causar problemas de vanishing gradients y exploding gradient.\n",
        "En cambio, un numero menor al optimo de epochs puede hacer que el modelo no aprenda lo suficiente.\n",
        "\n",
        "Una buena practica es ir aumentando las epochs hasta que la presición con los datos de validación empiece a decrecer (aún cuando la presicion con los datos de entrenamiento siga mejorando).\n"
      ],
      "metadata": {
        "id": "cEV4XdvM412i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CbjZ0eYW6k4P"
      }
    }
  ]
}