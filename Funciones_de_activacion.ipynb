{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Funciones-de-activacion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPAGN5oWVbxSt0S+bmr7Tup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aguscura/Python-Deep-Learning/blob/main/Funciones_de_activacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funciones de activación"
      ],
      "metadata": {
        "id": "Fci2qeTd9vqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La funcion de activación sirve para introducir la NO LINEALIDAD en el modelado de la red neuronal. \n",
        "\n",
        "Ejemplos:\n",
        "\n",
        "**Lineal** Justamente es la funcion lineal, \"identidad\", donde la señal de entrada no cambia al atravesar la funcion de activacion. Se usa cuando no queremos afectar la salida de una neurona.\n",
        "\n",
        "**Sigmoid** Permite reducir valores extremos de entrada en un rango entre 0 y 1. Es como si estuviera normalizando. La mayor cantidad de los valores de salida estaran siempre cerca de 0 o 1 debido a su forma asintotica bastante rapida. \n",
        "\n",
        "**Tanh** Es la tangente hiperbolica (senh(x) / cosh(x). A diferencia de sigmoid, el rango normalizado está entre -1 y 1. \n",
        "\n",
        "**Softmax** Esta funcion de activacion devuelve la distribucion de probabilidad sobre clases de salida mutuamente excluyentes. Por eso, en general, se usa en la ultima capa de una red. Para que justamente devuelva la probabilidad de cada clase. \n",
        "\n",
        "**ReLU** Rectified Linear Unit. Es una funcion interesante, donde la neurona se activa solo si el valor de entrada está por encima de un cierto umbral. El umbral mas usado es el valor 0. Entonces, toda entrada negativa tendra una salida igual a 0. Si el valor de entrada es positivo, la salida se verá afectada con una funcion lineal. Si la pendiente de esta recta es 1, el valor de salida es igual al de entrada."
      ],
      "metadata": {
        "id": "FhRTGbJc9zdY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7XRrIWS9sFO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}